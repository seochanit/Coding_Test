{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvqY8fnWCXwYSDjfs8Mmlv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seochanit/Coding_Test/blob/main/project_optim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5rszaw-F68vN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, BatchNormalization, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "class PreprocessData:\n",
        "    def __init__(self, valid_size, random_state, scaling=False):\n",
        "        self.valid_size = valid_size\n",
        "        self.random_state = random_state\n",
        "        self.scaling = scaling\n",
        "\n",
        "    def load_datasets(self):\n",
        "        (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "        return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "    def scaled_pixels(self, images, labels):\n",
        "        if self.scaling:\n",
        "            images = np.array(images / 255.0, dtype=np.float32)\n",
        "        else:\n",
        "            images = np.array(images, dtype=np.float32)\n",
        "        labels = np.array(labels, dtype=np.float32)\n",
        "        return images, labels\n",
        "\n",
        "    def transform_ohe(self, labels):\n",
        "        ohe_labels = to_categorical(labels)\n",
        "        return ohe_labels\n",
        "\n",
        "    def split_train_valid(self, train_images, train_ohe_labels):\n",
        "        tr_images, val_images, tr_ohe_labels, val_ohe_labels = train_test_split(train_images, train_ohe_labels,\n",
        "                                                                                test_size=self.valid_size,\n",
        "                                                                                random_state=self.random_state)\n",
        "        return tr_images, val_images, tr_ohe_labels, val_ohe_labels\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        # load dataset\n",
        "        train_images, train_labels, test_images, test_labels = self.load_datasets()\n",
        "        # convert to float32(not scaling)\n",
        "        train_images, train_labels = self.scaled_pixels(train_images, train_labels)\n",
        "        test_images, test_labels = self.scaled_pixels(test_images, test_labels)\n",
        "        # transform labels into One-hot encoding\n",
        "        train_ohe_labels = self.transform_ohe(train_labels)\n",
        "        test_ohe_labels = self.transform_ohe(test_labels)\n",
        "        # split train, valid data\n",
        "        tr_images, val_images, tr_ohe_labels, val_ohe_labels = self.split_train_valid(train_images, train_ohe_labels)\n",
        "        # check shape\n",
        "        print('Train:', tr_images.shape, tr_ohe_labels.shape)\n",
        "        print('Valid:', val_images.shape, val_ohe_labels.shape)\n",
        "        print('Test:', test_images.shape, test_ohe_labels.shape)\n",
        "\n",
        "        return tr_images, tr_ohe_labels, val_images, val_ohe_labels, test_images, test_ohe_labels\n",
        "\n",
        "\n",
        "class CnnModel:\n",
        "    input_size = 32\n",
        "\n",
        "    @classmethod\n",
        "    def change_input_size(cls, input_size):\n",
        "        CnnModel.input_size = input_size\n",
        "\n",
        "    @staticmethod\n",
        "    def create_model(verbose=True):\n",
        "        size = CnnModel.input_size\n",
        "        input_tensor = Input(shape=(size, size, 3))\n",
        "        # Block1\n",
        "        x = Conv2D(filters=32, kernel_size=3, padding='same', kernel_initializer='he_normal', activation='relu')(\n",
        "            input_tensor)\n",
        "        x = Conv2D(filters=32, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = MaxPooling2D(pool_size=2)(x)\n",
        "        # Block2\n",
        "        x = Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal', activation='relu')(x)\n",
        "        x = Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = MaxPooling2D(pool_size=2)(x)\n",
        "        # Block3\n",
        "        x = Conv2D(filters=128, kernel_size=3, padding='valid', kernel_initializer='he_normal', activation='relu')(x)\n",
        "        x = Conv2D(filters=128, kernel_size=3, padding='same', kernel_initializer='he_normal')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        x = MaxPooling2D(pool_size=2)(x)\n",
        "        # Block4\n",
        "        x = Conv2D(filters=256, kernel_size=3, strides=2, padding='same', kernel_initializer='he_normal')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "        # Classfier Layer\n",
        "        x = Flatten()(x)\n",
        "        x = Dropout(rate=0.4)(x)\n",
        "        x = Dense(units=256, kernel_initializer='he_normal', activation='relu')(x)\n",
        "        x = Dropout(rate=0.3)(x)\n",
        "        x = Dense(units=64, kernel_initializer='he_normal', activation='relu')(x)\n",
        "        output = Dense(units=10, activation='softmax')(x)\n",
        "\n",
        "        model = Model(inputs=input_tensor, outputs=output)\n",
        "        if verbose:\n",
        "            model.summary()\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "datasets = PreprocessData(vaild_size=0.15, random_state=42, scaling=False)\n",
        "tr_images, tr_ohe_labels, val_images, val_ohe_labels, test_images, test_ohe_labels = datasets.preprocess_data()\n",
        "\n",
        "model = CnnModel.create_model(verbose=True)\n",
        "\n",
        "tr_gen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rescale=1/255.0, rotation_range=0.45, zoom_range=[0.5,1.5])\n",
        "\n",
        "val_gen = ImageDataGenerator(rescale=1/255.0)\n",
        "\n",
        "flow_tr_gen = tr_gen.flow(x=tr_images, y=tr_ohe_labels, batch_size=64, shuffle=True)\n",
        "flow_val_gen = val_gen.flow(x=val_images, y=val_ohe_labels, batch_size=64, shuffle=False)\n",
        "\n",
        "rlr_call = ReduceLROnPlateau(monitor='val_loss', mode='min', factor=0.1, patience=4, verbose=1)\n",
        "es_call = EarlyStopping(monitor='val_loss', mode='min', patience=7, verbose=1)\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "tr_hist = model.fit(flow_tr_gen, epochs=20, validation_data=flow_val_gen, callbacks=[rlr_call, es_call])\n",
        "\n",
        "test_gen = ImageDataGenerator(rescale=1/255.0)\n",
        "flow_test_gen = test_gen.flow(x=test_images, y=test_ohe_labels, batch_size=32, shuffle=False)\n",
        "test_hist = model.evaluate(flow_test_gen)"
      ],
      "metadata": {
        "id": "sCO8Yn9F8lFn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}